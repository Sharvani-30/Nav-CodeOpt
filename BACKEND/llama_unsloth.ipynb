{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict API using Flask and finetuned Unsloth llama3.2B Model üñ•Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies for `LLM model`\n",
    "!pip install -U bitsandbytes\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "# install dependencies for `Flask` \n",
    "!pip install flask_cors\n",
    "!pip install pyngrok\n",
    "!pip install flask\n",
    "!pip install request\n",
    "!pip install jsonify\n",
    "!pip install threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Flask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from pyngrok import ngrok\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGROK_AUTH_KEY = \"\" # replace with valid NGROK auth key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask App Initialization\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for all routes\n",
    "\n",
    "# Ngrok setup\n",
    "ngrok.set_auth_token(NGROK_AUTH_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "model_path = \"G-210/code_optimization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_context = []\n",
    "\n",
    "def process_query(query):\n",
    "    \"\"\"\n",
    "    Process the query by appending it to the user context.\n",
    "    Generates a new result based on the accumulated context.\n",
    "    \"\"\"\n",
    "    # Get the current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Append the new query and the current date to the user context\n",
    "    user_context.append(f\"{current_date}\\nUser: {query}\")\n",
    "    full_context = \"\\n\".join(user_context)\n",
    "\n",
    "    # Prepare inputs for the model\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": full_context},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output\n",
    "    output_tokens = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=128000,  # Adjust token length as needed\n",
    "        use_cache=True,\n",
    "        temperature=0.8,\n",
    "        min_p=0.1,\n",
    "    )\n",
    "\n",
    "    # Decode the output tokens\n",
    "    raw_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    # Filter out unwanted phrases\n",
    "    unwanted_phrases = [\n",
    "        \"system\"\n",
    "        \"Cutting Knowledge Date:\",  # Add any phrases to exclude\n",
    "        \"Today Date:\",\n",
    "        \"December 2023\",\n",
    "        \"26 July 2024\",\n",
    "        \"user\"\n",
    "    ]\n",
    "    for phrase in unwanted_phrases:\n",
    "        raw_output = raw_output.replace(phrase, \"\").strip()\n",
    "\n",
    "    # Append only the user's query to the context to avoid repetition\n",
    "    return raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Endpoint to handle predictions based on user input.\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    user_input = data.get(\"i+nput\", \"\")\n",
    "\n",
    "    if not user_input:\n",
    "        return jsonify({\"error\": \"No input provided\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Generate a response using the model\n",
    "        response = process_query(user_input)\n",
    "        return jsonify({\"prediction\": response})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Ngrok tunnel and Flask app\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f\"Ngrok tunnel is running at {public_url}\")\n",
    "app.run(host=\"0.0.0.0\", port=5000, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP+QWna/cIR1qm5eoqfEE8b",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
